{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyODqA7ZvtSb1y93u2tG20/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helloKH/Psypy/blob/master/Scrapy_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eEguLd_I8fX",
        "colab_type": "text"
      },
      "source": [
        "scrapy를 이용한 웹 크롤링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3DmfP_YKK-d",
        "colab_type": "text"
      },
      "source": [
        "1. 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tWUseeSI5Jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install scrapy #설치"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_IO9VHPJ0xA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p95up6nyMnUB",
        "colab_type": "text"
      },
      "source": [
        "2. 공식 예제해보기\n",
        "\n",
        "- 출처: https://wjdcjf0219.tistory.com/33\n",
        "- 1.5 버전의 공식 예제는 http://quotes.toscrape.com 사이트의 링크를 순회하며 text와 authon를 스크레이핑하는 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71fUaeFwKYZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QuotesSpider(scrapy.Spider):\n",
        "    # spider의 이름(변경가능)\n",
        "    name = \"quotes\"\n",
        "    # 크롤링을 시작할 URL 리스트\n",
        "    start_urls = [\n",
        "        'http://quotes.toscrape.com/tag/humor/',\n",
        "    ]\n",
        "\n",
        "    def parse(self, response):\n",
        "        '''\n",
        "        링크를 순회하며 div.quote부분의 text와 author를 추출\n",
        "        '''\n",
        "        for quote in response.css('div.quote'):\n",
        "            yield {\n",
        "                'text': quote.css('span.text::text').extract_first(),\n",
        "                'author': quote.xpath('span/small/text()').extract_first(),\n",
        "            }\n",
        "\n",
        "        '''\n",
        "        최상위 페이지의 모든 링크를 추출\n",
        "        '''    \n",
        "        next_page = response.css('li.next a::attr(\"href\")').extract_first()\n",
        "        if next_page is not None:\n",
        "            yield response.follow(next_page, self.parse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSTg8UVYM_la",
        "colab_type": "text"
      },
      "source": [
        "scrapy의 runspider 명령어의 파라미터로 [실행 파일 경로, 출력 형태]를 지정하여 실행하면 로그와 함께 크롤링이 완료된 것을 볼 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UEbF0gKMfoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "e7f19a53-9942-4531-bdb9-2682d43c5597"
      },
      "source": [
        "scrapy runspider quotes_spider.py -o quotes.json"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-d1df26519e37>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    scrapy runspider quotes_spider.py -o quotes.json\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmVkV6ohNC5o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "6aaf0450-6450-4759-af2f-b4bb844d7978"
      },
      "source": [
        "C:\\workspace\\python\\scrapy>scrapy runspider quotes_spider.py -o quotes.json"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-d46353b89851>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    C:\\workspace\\python\\scrapy>scrapy runspider quotes_spider.py -o quotes.json\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfEPH0PfNHCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}